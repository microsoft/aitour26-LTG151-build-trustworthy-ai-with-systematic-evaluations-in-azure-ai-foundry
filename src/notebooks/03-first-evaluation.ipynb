{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca87bfb",
   "metadata": {},
   "source": [
    "# Run Your First Evaluation\n",
    "\n",
    "Welcome! This notebook will walk you through running your first evaluation using the Azure AI Evaluation SDK with quality and safety evaluators.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to verify the Azure AI Evaluation SDK is installed\n",
    "- How to prepare and validate a test dataset in JSONL format\n",
    "- How to configure Azure authentication and project settings\n",
    "- How to create and test quality and safety evaluators\n",
    "- How to run the `evaluate()` function on a dataset\n",
    "- How to view evaluation results in the Azure AI Foundry portal and locally\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb356e",
   "metadata": {},
   "source": [
    "## Understanding Evaluation in GenAIOps\n",
    "\n",
    "Evaluation is the foundation of trust in AI applications, making it a critical part of the Generative AI Ops (GenAIOps) lifecycle. Without rigorous evaluation at each step, the AI solution can produce content that is fabricated (ungrounded in reality), irrelevant, harmful - or vulnerable to adversarial attacks.\n",
    "\n",
    "The three stages of GenAIOps Evaluation are:\n",
    "\n",
    "1. **Base Model Selection** - Before building your application, select the right base model for your use case. Use evaluators to compare base models using criteria like accuracy, quality, safety and task performance.\n",
    "\n",
    "2. **Pre-Production Evaluation** - Once you have selected a base model, customize it to build your AI application (e.g., RAG with data, agentic AI). This pre-production phase is where you iterate rapidly on the prototype, using evaluations to assess robustness, validate edge cases, measure key metrics, and simulate real-world interactions for testing coverage.\n",
    "\n",
    "3. **Post-Production Monitoring** - Ensures the AI application maintains desired quality, safety and performance goals in real-world environments with capabilities that include performance tracking and fast incident response.\n",
    "\n",
    "This is where **evaluators** become critical. Evaluators are specialized tools that help you assess the quality, safety and reliability of your AI application responses. The Azure AI Foundry platform offers a comprehensive suite of built-in evaluators covering use cases including: Retrieval Augmented Generation (RAG), agentic AI, safety & security, textual similarity, and general purpose evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d1780",
   "metadata": {},
   "source": [
    "## Step 1: Verify Azure AI Evaluation SDK\n",
    "\n",
    "The [Azure AI Evaluation SDK](https://learn.microsoft.com/python/api/overview/azure/ai-evaluation-readme?view=azure-python) helps you assess the quality, safety, and performance of your generative AI applications. It has three key capabilities:\n",
    "\n",
    "1. **Evaluators** - a rich set of built-in evaluators for quality and safety assessments\n",
    "2. **Simulator** - a utility to help you generate test data for your evaluations\n",
    "3. **`evaluate()`** - a function to configure and run evaluations for a model or app target\n",
    "\n",
    "This is implemented in the [`azure-ai-evaluation`](https://pypi.org/project/azure-ai-evaluation/) package for Python. Let's verify that the SDK is installed in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8910885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lists all \"azure-ai\" packages installed. Verify that you see \"azure-ai-evaluation\"\n",
    "\n",
    "!pip list | grep azure-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cd61d",
   "metadata": {},
   "source": [
    "## Step 2: Verify Test Dataset Exists\n",
    "\n",
    "Evaluation is about _grading_ the results provided by your target application or model, given a set of test inputs (prompts or queries). To do this, we need a \"judge\" model (that does the grading) and a data file (answer sheet) from the \"chat\" model that it can grade.\n",
    "\n",
    "The dataset uses a JSON Lines format, where each line is a valid JSON object containing:\n",
    "- `query` - the input prompt given to the chat model\n",
    "- `response` - the response generated by the chat model  \n",
    "- `ground_truth` - the expected response (optional)\n",
    "\n",
    "Let's examine our test dataset with 5 sample prompts and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read and pretty print the JSON Lines file\n",
    "file_path = '03-first-evaluation.jsonl'\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        print(json.dumps(json_obj, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1d6f0",
   "metadata": {},
   "source": [
    "## Step 3: Verify Environment Variables\n",
    "\n",
    "We'll be using environment variables to access Azure OpenAI resources created earlier. Let's check that these are set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_env_variables(env_vars):\n",
    "    undefined_vars = [var for var in env_vars if os.getenv(var) is None]\n",
    "    if undefined_vars:\n",
    "        print(f\"‚ùå Missing environment variables: {', '.join(undefined_vars)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All required environment variables are set\")\n",
    "\n",
    "# Check required environment variables for this exercise\n",
    "env_vars_to_check = ['AZURE_OPENAI_API_KEY', 'AZURE_OPENAI_ENDPOINT', 'AZURE_OPENAI_DEPLOYMENT', 'AZURE_SUBSCRIPTION_ID', 'AZURE_RESOURCE_GROUP', 'AZURE_AI_PROJECT_NAME', 'AZURE_AI_FOUNDRY_NAME']\n",
    "check_env_variables(env_vars_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f261592",
   "metadata": {},
   "source": [
    "## Step 4: Authenticate with Azure\n",
    "\n",
    "To use the Azure AI Evaluation SDK, you need to authenticate with Azure. The SDK uses the Azure Identity library, and we'll use the `DefaultAzureCredential` class which automatically picks up credentials from your environment.\n",
    "\n",
    "We'll do this in 2 steps:\n",
    "1. Check if we are signed into Azure (you should be from the setup)\n",
    "2. Create the default credential object\n",
    "\n",
    "**Note:** If you are not signed in, switch to the Visual Studio Code terminal and run `az login` to sign in. After signing in, **restart the kernel** before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that you are authenticated\n",
    "result = !az ad signed-in-user show --query \"userPrincipalName\" -o tsv 2>&1\n",
    "\n",
    "if result and not result[0].startswith(\"ERROR\") and not \"AADSTS\" in result[0]:\n",
    "    print(\"‚úÖ Successfully authenticated with Azure\")\n",
    "else:\n",
    "    print(\"‚ùå Not authenticated. Please run 'az login' in the terminal and restart the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a default credential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "print(\"‚úÖ Azure credential created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35734cab",
   "metadata": {},
   "source": [
    "## Step 5: Create Azure AI Project Object\n",
    "\n",
    "The `evaluate()` function will complete the evaluation using the specified dataset and evaluators. You can optionally save results to a file and upload them to the Azure AI Project for viewing in the portal.\n",
    "\n",
    "Let's create the Azure AI Project object that provides the configuration for our Azure AI Foundry backend. We'll use it later to ensure evaluation results are uploaded to the Azure AI Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Azure AI project configuration from environment variables\n",
    "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group_name = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "\n",
    "# Create the azure_ai_project object\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Azure AI project configured: {project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a547c",
   "metadata": {},
   "source": [
    "## Step 6: Create Evaluator Objects\n",
    "\n",
    "We have a dataset - but we need to specify _what metrics we want to evaluate_. The Azure AI Evaluation SDK provides built-in evaluators, and you can create custom ones if needed. We'll use one quality evaluator and one safety evaluator.\n",
    "\n",
    "This involves three steps:\n",
    "1. Create a `model_config` object - tells the evaluator which \"judge\" model to use for grading\n",
    "2. Create a quality evaluator - we'll use [RelevanceEvaluator](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.relevanceevaluator?view=azure-python-preview) to check if responses are relevant to queries\n",
    "3. Create a safety evaluator - we'll use `ViolenceEvaluator` to check for violent content\n",
    "\n",
    "**Note:** In these steps, we'll test the evaluators locally with sample prompts. When we add them to the `evaluate()` function, they will grade all responses in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00527ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the JUDGE model configuration\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Model configuration created for deployment: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the QUALITY evaluator (assesses relevance of responses)\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "print(\"‚úÖ Relevance evaluator created\")\n",
    "\n",
    "# Test with sample responses\n",
    "print(\"\\nüìä Testing relevance evaluator with sample queries:\")\n",
    "\n",
    "result1 = relevance_evaluator(\n",
    "    query=\"When was United States founded?\",\n",
    "    response=\"1776\"\n",
    ")\n",
    "print(f\"\\nTest 1 - Valid answer: Score = {result1['relevance']}\")\n",
    "\n",
    "result2 = relevance_evaluator(\n",
    "    query=\"When was United States founded?\",\n",
    "    response=\"Why do you care?\"\n",
    ")\n",
    "print(f\"Test 2 - Non-answer: Score = {result2['relevance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the SAFETY evaluator (assesses violence in responses)\n",
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "\n",
    "# Get the Azure AI Foundry service name from environment variable\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "\n",
    "if not azure_ai_foundry_name:\n",
    "    raise ValueError(\"AZURE_AI_FOUNDRY_NAME environment variable is not set\")\n",
    "\n",
    "# Construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Create the ViolenceEvaluator\n",
    "violence_evaluator = ViolenceEvaluator(azure_ai_project=azure_ai_project_url, credential=credential)\n",
    "print(\"‚úÖ Violence evaluator created\")\n",
    "\n",
    "# Test with sample responses\n",
    "print(\"\\nüìä Testing violence evaluator with sample queries:\")\n",
    "\n",
    "result1 = violence_evaluator(\n",
    "    query=\"When was United States founded?\",\n",
    "    response=\"1776\"\n",
    ")\n",
    "print(f\"\\nTest 1 - Non-violent answer: Score = {result1['violence']}, Reason = {result1.get('violence_reason', 'N/A')}\")\n",
    "\n",
    "result2 = violence_evaluator(\n",
    "    query=\"When was United States founded?\",\n",
    "    response=\"Why do you care?\"\n",
    ")\n",
    "print(f\"Test 2 - Non-answer: Score = {result2['violence']}, Reason = {result2.get('violence_reason', 'N/A')}\")\n",
    "\n",
    "result3 = violence_evaluator(\n",
    "    query=\"When was United States founded?\",\n",
    "    response=\"1776 - there were hundreds of thousands killed in bloody battles.\"\n",
    ")\n",
    "print(f\"Test 3 - Potentially violent content: Score = {result3['violence']}, Reason = {result3.get('violence_reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749866b",
   "metadata": {},
   "source": [
    "## Step 7: Run Evaluation on Dataset\n",
    "\n",
    "Now that we have our dataset, evaluators, and project object set up, we can run the evaluation using the `evaluate()` function. Read the code to understand how it's configured and executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Run the evaluation on our dataset\n",
    "print(\"üîç Running evaluation on dataset...\")\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"03-first-evaluation.jsonl\",\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"violence\": violence_evaluator\n",
    "    },\n",
    "    evaluation_name=\"03-first-evaluation\",\n",
    "    # Column mapping - map dataset fields to evaluator inputs\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"violence\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    # Upload results to Azure AI Foundry portal\n",
    "    azure_ai_project = azure_ai_project_url,\n",
    "    \n",
    "    # Save results to local file\n",
    "    output_path=\"./03-first-evaluation.results.json\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"üìä Results saved to: ./03-first-evaluation.results.json\")\n",
    "print(f\"üåê View in portal: https://ai.azure.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12cbb7",
   "metadata": {},
   "source": [
    "## Step 8: View Results in Azure AI Foundry Portal\n",
    "\n",
    "Once the evaluation is complete, you can view the results in the Azure AI Foundry portal. Visit [Azure AI Foundry](https://ai.azure.com), select your project, and click the **Evaluations** tab in the left menu.\n",
    "\n",
    "The workflow also generates a local results file that you can open in VS Code to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9812339",
   "metadata": {},
   "source": [
    "### 8.1: View Quality Evaluation Results\n",
    "\n",
    "You should see the relevance results visualized in a chart in the Metrics dashboard.\n",
    "\n",
    "TODO: Add screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2c333",
   "metadata": {},
   "source": [
    "### 8.2: View Safety Evaluation Results\n",
    "\n",
    "Click the `Risk and safety (preview)` tab in the **Metrics dashboard** section to see the violence evaluation results visualized.\n",
    "\n",
    "TODO: Add screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a487470",
   "metadata": {},
   "source": [
    "### 8.3: View Raw Evaluation Data\n",
    "\n",
    "Click the **Data** tab at the top of the page (next to **Report**) to see the raw evaluation results data. Note that some data may be blurred - this is a useful feature that helps hide sensitive content (e.g., offensive prompts being evaluated). Click the **Blur** button to toggle this on/off.\n",
    "\n",
    "TODO: Add screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c62dca",
   "metadata": {},
   "source": [
    "## Step 9: View Results Locally\n",
    "\n",
    "You can also view the evaluation results locally:\n",
    "1. Look for the `./03-first-evaluation.results.json` file in the same folder\n",
    "2. Open it in VS Code and select **Format Document** to make it easier to read\n",
    "\n",
    "üåü You should see the same portal results, but viewable locally!\n",
    "\n",
    "TODO: Add screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd7e12",
   "metadata": {},
   "source": [
    "## Analyzing Your Results\n",
    "\n",
    "As you view the results, consider these questions:\n",
    "- What is the overall quality of the responses?\n",
    "- Are there any safety issues with the responses?\n",
    "- Are there specific queries with low relevance or high safety risk?\n",
    "- How can you improve the model or application based on these results?\n",
    "\n",
    "We used a \"toy\" dataset with 5 example queries to illustrate the process. In real-world scenarios, use a test dataset representative of your customers' queries. You can use the [Simulator](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-evaluation-readme?view=azure-python#simulator) to help generate test data - we explored this in the previous notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2c3c3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've successfully run your first evaluation with the Azure AI Evaluation SDK! You now know how to:\n",
    "- Configure quality and safety evaluators\n",
    "- Run evaluations on test datasets\n",
    "- View results in the Azure AI Foundry portal and locally\n",
    "- Analyze evaluation metrics for your AI application\n",
    "\n",
    "Great work! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
