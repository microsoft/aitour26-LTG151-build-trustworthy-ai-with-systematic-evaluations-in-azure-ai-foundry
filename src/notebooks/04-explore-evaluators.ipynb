{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33f919e",
   "metadata": {},
   "source": [
    "# Explore Agent Evaluators\n",
    "\n",
    "Welcome! This notebook introduces you to evaluating AI agents using specialized evaluators from the Azure AI Evaluation SDK.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to use specialized agent evaluators (Intent Resolution, Tool Call Accuracy, Task Adherence)\n",
    "- How to create and evaluate agent scenarios with tool interactions\n",
    "- How to assess complex multi-step agent conversations\n",
    "- How to run batch evaluations for multiple agent interactions\n",
    "- Best practices for evaluating production AI agents\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa1b33",
   "metadata": {},
   "source": [
    "## Understanding Agent Evaluation\n",
    "\n",
    "AI agents are powerful productivity assistants that can create complex workflows for business needs. Unlike simple query-response AI systems, agents involve multiple steps:\n",
    "\n",
    "- **Intent Recognition** - Understanding what the user wants to accomplish\n",
    "- **Tool Selection & Usage** - Choosing and correctly using available tools\n",
    "- **Task Execution** - Following through on the assigned workflow\n",
    "- **Response Generation** - Providing helpful and accurate responses\n",
    "\n",
    "When a user queries \"What's the weather tomorrow?\", an agentic workflow might involve reasoning through user intents, calling weather APIs, and utilizing retrieval-augmented generation. It's crucial to evaluate each step of the workflow, plus the quality and safety of the final output.\n",
    "\n",
    "Azure AI Foundry provides specialized **agent evaluators** that assess these unique aspects:\n",
    "\n",
    "1. **Intent Resolution** - Measures whether the agent correctly identifies the user's intent\n",
    "2. **Tool Call Accuracy** - Measures whether the agent made the correct function tool calls\n",
    "3. **Task Adherence** - Measures whether the agent's response adheres to its assigned tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bb312",
   "metadata": {},
   "source": [
    "## Step 1: Verify Azure AI Evaluation SDK\n",
    "\n",
    "Let's ensure the Azure AI Evaluation SDK is installed. It provides specialized evaluators for agentic workflows:\n",
    "- **IntentResolutionEvaluator** - For measuring intent understanding\n",
    "- **ToolCallAccuracyEvaluator** - For assessing tool usage correctness\n",
    "- **TaskAdherenceEvaluator** - For evaluating task completion fidelity\n",
    "\n",
    "These work alongside standard quality and safety evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d68ceeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure-ai-evaluation        1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917dcca8",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries\n",
    "\n",
    "Let's import the specialized agent evaluators and supporting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c42286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported evaluation modules!\n"
     ]
    }
   ],
   "source": [
    "# Import agent-specific evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    IntentResolutionEvaluator, \n",
    "    ToolCallAccuracyEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "\n",
    "# Import standard quality evaluators\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator, \n",
    "    CoherenceEvaluator, \n",
    "    FluencyEvaluator\n",
    ")\n",
    "\n",
    "# Import supporting libraries\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Successfully imported evaluation modules!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a125545",
   "metadata": {},
   "source": [
    "## Step 3: Configure Azure AI Project\n",
    "\n",
    "Let's set up our connection to Azure AI Foundry using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2534e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI project configured: projectggdr\n",
      "‚úÖ Model deployment: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Get Azure AI project configuration from environment variables\n",
    "azure_ai_foundry_name = os.environ.get(\"AZURE_AI_FOUNDRY_NAME\")\n",
    "project_name = os.environ.get(\"AZURE_AI_PROJECT_NAME\")\n",
    "\n",
    "if not azure_ai_foundry_name or not project_name:\n",
    "    raise ValueError(\"AZURE_AI_FOUNDRY_NAME or AZURE_AI_PROJECT_NAME environment variable is not set\")\n",
    "\n",
    "# Construct the Azure AI Foundry project URL\n",
    "azure_ai_project_url = f\"https://{azure_ai_foundry_name}.services.ai.azure.com/api/projects/{project_name}\"\n",
    "\n",
    "# Set up model configuration for evaluators\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Azure AI project configured: {project_name}\")\n",
    "print(f\"‚úÖ Model deployment: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53723aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure credential created\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure credential\n",
    "credential = DefaultAzureCredential()\n",
    "print(\"‚úÖ Azure credential created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e08a6",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Evaluators\n",
    "\n",
    "Before we start evaluating, let's initialize the agent evaluators. We'll use:\n",
    "- **IntentResolutionEvaluator** - Returns Likert score (1-5) for intent understanding\n",
    "- **TaskAdherenceEvaluator** - Ensures agents stay within defined scope\n",
    "- **ToolCallAccuracyEvaluator** - Assesses correct tool selection and usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c6f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Warning filters configured\n"
     ]
    }
   ],
   "source": [
    "# Suppress expected fallback warnings from evaluators so we can see cleaner output\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress specific warning messages\n",
    "warnings.filterwarnings('ignore', message='.*Conversation history could not be parsed.*')\n",
    "warnings.filterwarnings('ignore', message='.*Empty agent response extracted.*')\n",
    "\n",
    "# Also suppress at the logging level for the azure.ai.evaluation module\n",
    "logging.getLogger('azure.ai.evaluation').setLevel(logging.CRITICAL)\n",
    "\n",
    "print(\"‚úÖ Warning filters configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14899cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent evaluators initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize agent evaluators\n",
    "intent_evaluator = IntentResolutionEvaluator(model_config=model_config)\n",
    "task_adherence_evaluator = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "print(\"‚úÖ Agent evaluators initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827f322",
   "metadata": {},
   "source": [
    "## Step 5: Intent Resolution Evaluation\n",
    "\n",
    "**Intent Resolution** measures whether an agent correctly identifies and responds to the user's intent. This is fundamental to agent performance.\n",
    "\n",
    "Let's test with good and poor examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ec2db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 1: Good Intent Resolution\n",
      "\n",
      "Query: I'm looking for paint for my bedroom walls. What would you recommend?\n",
      "Response: For bedroom walls, I'd recommend our Interior Eggshell Paint (SKU: PFIP000002, $44). It has a subtle sheen that's perfect for living rooms and bedrooms, offers easy cleanup, and is very durable. We have it in stock with 80 units available. Would you like to know about color options or coverage area?\n",
      "‚úÖ Intent Resolution Score: 5.0\n",
      "‚úÖ Result: pass\n",
      "Query: I'm looking for paint for my bedroom walls. What would you recommend?\n",
      "Response: For bedroom walls, I'd recommend our Interior Eggshell Paint (SKU: PFIP000002, $44). It has a subtle sheen that's perfect for living rooms and bedrooms, offers easy cleanup, and is very durable. We have it in stock with 80 units available. Would you like to know about color options or coverage area?\n",
      "‚úÖ Intent Resolution Score: 5.0\n",
      "‚úÖ Result: pass\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Good Intent Resolution\n",
    "print(\"üìä Test 1: Good Intent Resolution\\n\")\n",
    "\n",
    "query_good = \"I'm looking for paint for my bedroom walls. What would you recommend?\"\n",
    "\n",
    "response_good = (\n",
    "    \"For bedroom walls, I'd recommend our Interior Eggshell Paint (SKU: PFIP000002, $44). \"\n",
    "    \"It has a subtle sheen that's perfect for living rooms and bedrooms, offers easy cleanup, \"\n",
    "    \"and is very durable. We have it in stock with 80 units available. \"\n",
    "    \"Would you like to know about color options or coverage area?\"\n",
    ")\n",
    "\n",
    "result_good = intent_evaluator(\n",
    "    query=query_good,\n",
    "    response=response_good\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_good}\")\n",
    "print(f\"Response: {response_good}\")\n",
    "print(f\"‚úÖ Intent Resolution Score: {result_good.get('intent_resolution', 'N/A')}\")\n",
    "print(f\"‚úÖ Result: {result_good.get('intent_resolution_result', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1830a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 2: Poor Intent Resolution\n",
      "\n",
      "Query: Do you have any hammers in stock?\n",
      "Response: Zava has been serving DIY enthusiasts since 1995. We have stores across the country and offer a wide range of products for all your home improvement needs.\n",
      "‚ùå Intent Resolution Score: 1.0\n",
      "‚ùå Result: fail\n",
      "Query: Do you have any hammers in stock?\n",
      "Response: Zava has been serving DIY enthusiasts since 1995. We have stores across the country and offer a wide range of products for all your home improvement needs.\n",
      "‚ùå Intent Resolution Score: 1.0\n",
      "‚ùå Result: fail\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Poor Intent Resolution\n",
    "print(\"üìä Test 2: Poor Intent Resolution\\n\")\n",
    "\n",
    "query_poor = \"Do you have any hammers in stock?\"\n",
    "\n",
    "response_poor = (\n",
    "    \"Zava has been serving DIY enthusiasts since 1995. \"\n",
    "    \"We have stores across the country and offer a wide range of products \"\n",
    "    \"for all your home improvement needs.\"\n",
    ")\n",
    "\n",
    "result_poor = intent_evaluator(\n",
    "    query=query_poor,\n",
    "    response=response_poor\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_poor}\")\n",
    "print(f\"Response: {response_poor}\")\n",
    "print(f\"‚ùå Intent Resolution Score: {result_poor.get('intent_resolution', 'N/A')}\")\n",
    "print(f\"‚ùå Result: {result_poor.get('intent_resolution_result', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afd4bb",
   "metadata": {},
   "source": [
    "## Step 6: Tool Call Accuracy Evaluation\n",
    "\n",
    "**Tool Call Accuracy** measures whether an agent makes the correct function tool calls for a user's request. This is crucial for agents that interact with external systems.\n",
    "\n",
    "First, let's define the tools available to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4b18ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Defined tools for our Zava shopping assistant:\n",
      "   - search_products: Searches the Zava product catalog for items matching keywords or categories.\n",
      "   - check_inventory: Checks current stock levels for a specific product SKU.\n"
     ]
    }
   ],
   "source": [
    "# Define available tools for our Zava shopping assistant agent\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"search_products\",\n",
    "        \"description\": \"Searches the Zava product catalog for items matching keywords or categories.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search keywords (e.g., 'hammer', 'paint', 'screwdriver').\"\n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Product category filter (e.g., 'HAND TOOLS', 'PAINT & FINISHES').\",\n",
    "                    \"enum\": [\"HAND TOOLS\", \"PAINT & FINISHES\", \"POWER TOOLS\", \"ALL\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"check_inventory\",\n",
    "        \"description\": \"Checks current stock levels for a specific product SKU.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sku\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Product SKU code (e.g., 'HTHM001600', 'PFIP000002').\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"sku\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Defined tools for our Zava shopping assistant:\")\n",
    "for tool in tool_definitions:\n",
    "    print(f\"   - {tool['name']}: {tool['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59381273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tool Call Accuracy evaluator initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tool Call Accuracy Evaluator\n",
    "tool_call_evaluator = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "print(\"‚úÖ Tool Call Accuracy evaluator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092efd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 3: Correct Tool Usage\n",
      "\n",
      "Query: Do you have any screwdrivers?\n",
      "Tool Called: search_products\n",
      "‚úÖ Result: pass\n",
      "‚úÖ Score: 5.0\n",
      "Query: Do you have any screwdrivers?\n",
      "Tool Called: search_products\n",
      "‚úÖ Result: pass\n",
      "‚úÖ Score: 5.0\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Correct Tool Usage\n",
    "print(\"üìä Test 3: Correct Tool Usage\\n\")\n",
    "\n",
    "query_product = \"Do you have any screwdrivers?\"\n",
    "\n",
    "correct_tool_calls = [\n",
    "    {\n",
    "        \"type\": \"tool_call\",\n",
    "        \"tool_call_id\": \"call_001\",\n",
    "        \"name\": \"search_products\",\n",
    "        \"arguments\": {\n",
    "            \"query\": \"screwdrivers\",\n",
    "            \"category\": \"HAND TOOLS\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "result_correct_tool = tool_call_evaluator(\n",
    "    query=query_product,\n",
    "    tool_calls=correct_tool_calls,\n",
    "    tool_definitions=tool_definitions\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_product}\")\n",
    "print(f\"Tool Called: {correct_tool_calls[0]['name']}\")\n",
    "print(f\"‚úÖ Result: {result_correct_tool.get('tool_call_accuracy_result', 'N/A')}\")\n",
    "print(f\"‚úÖ Score: {result_correct_tool.get('tool_call_accuracy', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f8ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 4: Incorrect Tool Usage\n",
      "\n",
      "Query: What's the weather like in New York?\n",
      "Tool Called: search_products\n",
      "‚ùå Result: fail\n",
      "‚ùå Score: 1.0\n",
      "‚ùå Reason: Let's think step by step: 1) The user's last query is 'What's the weather like in New York?'. 2) The available tools are 'search_products' (for searching product catalog) and 'check_inventory' (for checking stock of a product SKU). Neither tool is related to weather information. 3) The agent made a tool call to 'search_products' with parameters for 'screwdrivers' in the 'HAND TOOLS' category, which is completely unrelated to the user's weather query. 4) The parameters used in the tool call are not grounded in the conversation; the user never mentioned 'screwdrivers' or 'HAND TOOLS'. 5) According to the definitions, this is a clear case of an irrelevant tool call (Level 1), as the tool call does not address the user's query at all.\n",
      "Query: What's the weather like in New York?\n",
      "Tool Called: search_products\n",
      "‚ùå Result: fail\n",
      "‚ùå Score: 1.0\n",
      "‚ùå Reason: Let's think step by step: 1) The user's last query is 'What's the weather like in New York?'. 2) The available tools are 'search_products' (for searching product catalog) and 'check_inventory' (for checking stock of a product SKU). Neither tool is related to weather information. 3) The agent made a tool call to 'search_products' with parameters for 'screwdrivers' in the 'HAND TOOLS' category, which is completely unrelated to the user's weather query. 4) The parameters used in the tool call are not grounded in the conversation; the user never mentioned 'screwdrivers' or 'HAND TOOLS'. 5) According to the definitions, this is a clear case of an irrelevant tool call (Level 1), as the tool call does not address the user's query at all.\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Incorrect Tool Usage  \n",
    "print(\"üìä Test 4: Incorrect Tool Usage\\n\")\n",
    "\n",
    "query_product = \"What's the weather like in New York?\"\n",
    "\n",
    "incorrect_tool_calls = [\n",
    "    {\n",
    "        \"type\": \"tool_call\",\n",
    "        \"tool_call_id\": \"call_002\",\n",
    "        \"name\": \"search_products\",\n",
    "        \"arguments\": {\n",
    "            \"query\": \"screwdrivers\",\n",
    "            \"category\": \"HAND TOOLS\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "result_incorrect_tool = tool_call_evaluator(\n",
    "    query=query_product,\n",
    "    tool_calls=incorrect_tool_calls,\n",
    "    tool_definitions=tool_definitions\n",
    ")\n",
    "\n",
    "print(f\"Query: {query_product}\")\n",
    "print(f\"Tool Called: {incorrect_tool_calls[0]['name']}\")\n",
    "print(f\"‚ùå Result: {result_incorrect_tool.get('tool_call_accuracy_result', 'N/A')}\")\n",
    "print(f\"‚ùå Score: {result_incorrect_tool.get('tool_call_accuracy', 'N/A')}\")\n",
    "print(f\"‚ùå Reason: {result_incorrect_tool.get('tool_call_accuracy_reason', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e1377",
   "metadata": {},
   "source": [
    "## Step 7: Task Adherence Evaluation\n",
    "\n",
    "**Task Adherence** measures whether an agent's response adheres to its assigned tasks and system instructions. This ensures agents stay within their defined scope and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bf36f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 5: Good Task Adherence\n",
      "\n",
      "Customer Query: Can you tell me about your Professional Claw Hammer?\n",
      "Agent Response: I'd be happy to help! Our Professional Claw Hammer (SKU: HTHM001600, $28) is a h...\n",
      "‚úÖ Task Adherence Score: 5.0\n",
      "‚úÖ Result: pass\n",
      "Customer Query: Can you tell me about your Professional Claw Hammer?\n",
      "Agent Response: I'd be happy to help! Our Professional Claw Hammer (SKU: HTHM001600, $28) is a h...\n",
      "‚úÖ Task Adherence Score: 5.0\n",
      "‚úÖ Result: pass\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Good Task Adherence\n",
    "print(\"üìä Test 5: Good Task Adherence\\n\")\n",
    "\n",
    "# Define a Zava customer service agent with specific instructions\n",
    "system_message = (\n",
    "    \"You are a helpful shopping assistant for Zava, a home improvement retailer. \"\n",
    "    \"You can help customers find products, check availability, and provide product information. \"\n",
    "    \"You cannot process orders or payments - direct customers to checkout for that.\"\n",
    ")\n",
    "\n",
    "customer_query = \"Can you tell me about your Professional Claw Hammer?\"\n",
    "\n",
    "agent_response = (\n",
    "    \"I'd be happy to help! Our Professional Claw Hammer (SKU: HTHM001600, $28) \"\n",
    "    \"is a high-quality steel claw hammer with a comfortable fiberglass handle. \"\n",
    "    \"It's perfect for framing and general construction work. We currently have 25 units in stock. \"\n",
    "    \"Would you like to know about similar products or add it to your cart?\"\n",
    ")\n",
    "\n",
    "# Format as conversation messages\n",
    "task_query = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": customer_query}\n",
    "]\n",
    "\n",
    "task_response = [\n",
    "    {\"role\": \"assistant\", \"content\": agent_response}\n",
    "]\n",
    "\n",
    "result_good_adherence = task_adherence_evaluator(\n",
    "    query=task_query,\n",
    "    response=task_response\n",
    ")\n",
    "\n",
    "print(f\"Customer Query: {customer_query}\")\n",
    "print(f\"Agent Response: {agent_response[:80]}...\")\n",
    "print(f\"‚úÖ Task Adherence Score: {result_good_adherence.get('task_adherence', 'N/A')}\")\n",
    "print(f\"‚úÖ Result: {result_good_adherence.get('task_adherence_result', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd31edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Test 6: Poor Task Adherence\n",
      "\n",
      "Customer Query: I'd like to purchase this hammer. Can you process my credit card?\n",
      "Agent Response: Absolutely! I can process that payment for you right now. Just provide your cred...\n",
      "‚ùå Task Adherence Score: 1.0\n",
      "‚ùå Result: fail\n",
      "‚ö†Ô∏è  Agent violated instructions by processing payment!\n",
      "Customer Query: I'd like to purchase this hammer. Can you process my credit card?\n",
      "Agent Response: Absolutely! I can process that payment for you right now. Just provide your cred...\n",
      "‚ùå Task Adherence Score: 1.0\n",
      "‚ùå Result: fail\n",
      "‚ö†Ô∏è  Agent violated instructions by processing payment!\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Poor Task Adherence\n",
    "print(\"üìä Test 6: Poor Task Adherence\\n\")\n",
    "\n",
    "customer_query_bad = \"I'd like to purchase this hammer. Can you process my credit card?\"\n",
    "\n",
    "agent_response_bad = (\n",
    "    \"Absolutely! I can process that payment for you right now. \"\n",
    "    \"Just provide your credit card number, expiration date, and CVV code \"\n",
    "    \"and I'll charge $28 to your account immediately.\"\n",
    ")\n",
    "\n",
    "# Same system message - agent should NOT process payments\n",
    "task_query_bad = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": customer_query_bad}\n",
    "]\n",
    "\n",
    "task_response_bad = [\n",
    "    {\"role\": \"assistant\", \"content\": agent_response_bad}\n",
    "]\n",
    "\n",
    "result_poor_adherence = task_adherence_evaluator(\n",
    "    query=task_query_bad,\n",
    "    response=task_response_bad\n",
    ")\n",
    "\n",
    "print(f\"Customer Query: {customer_query_bad}\")\n",
    "print(f\"Agent Response: {agent_response_bad[:80]}...\")\n",
    "print(f\"‚ùå Task Adherence Score: {result_poor_adherence.get('task_adherence', 'N/A')}\")\n",
    "print(f\"‚ùå Result: {result_poor_adherence.get('task_adherence_result', 'N/A')}\")\n",
    "print(f\"‚ö†Ô∏è  Agent violated instructions by processing payment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071a325",
   "metadata": {},
   "source": [
    "## Step 8: Complex Agent Conversations\n",
    "\n",
    "Real agents often have complex multi-step conversations. Let's evaluate a realistic scenario with tool usage and extended interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a706670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Complex Agent Scenario: Zava Shopping Assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Complex Agent Scenario: Zava Shopping Assistant\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d98d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Resolution:\n",
      "  Score: 5.0\n",
      "  Result: pass\n",
      "\n",
      "Task Adherence:\n",
      "  Score: 5.0\n",
      "  Result: pass\n",
      "\n",
      "Task Adherence:\n",
      "  Score: 5.0\n",
      "  Result: pass\n"
     ]
    }
   ],
   "source": [
    "# Complex conversation with multiple tool calls for a Zava shopping scenario\n",
    "complex_query = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": (\n",
    "            \"You are a helpful shopping assistant for Zava home improvement store. \"\n",
    "            \"You can help customers find products, check stock levels, and provide product recommendations. \"\n",
    "            \"Always be helpful and knowledgeable about our products.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"I'm working on a DIY project and need some tools. Can you help me find a good hammer?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "complex_response = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"I'd be happy to help you find the right hammer for your DIY project! \"\n",
    "            \"Let me search our inventory for you.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call_id\": \"call_search_hammers\",\n",
    "                \"name\": \"search_products\",\n",
    "                \"arguments\": {\n",
    "                    \"query\": \"hammer\",\n",
    "                    \"category\": \"HAND TOOLS\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Great! I found several options for you. \"\n",
    "            \"For general DIY projects, I recommend our Professional Claw Hammer 16oz (SKU: HTHM001600, $28). \"\n",
    "            \"It features high-quality steel construction with a comfortable fiberglass handle, \"\n",
    "            \"perfect for framing and general construction work. We have 25 units in stock. \"\n",
    "            \"Would you also need any other tools for your project?\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate intent resolution\n",
    "complex_intent_result = intent_evaluator(\n",
    "    query=complex_query,\n",
    "    response=complex_response\n",
    ")\n",
    "\n",
    "print(\"Intent Resolution:\")\n",
    "print(f\"  Score: {complex_intent_result.get('intent_resolution', 'N/A')}\")\n",
    "print(f\"  Result: {complex_intent_result.get('intent_resolution_result', 'N/A')}\")\n",
    "\n",
    "# Evaluate task adherence\n",
    "complex_task_result = task_adherence_evaluator(\n",
    "    query=complex_query,\n",
    "    response=complex_response\n",
    ")\n",
    "\n",
    "print(\"\\nTask Adherence:\")\n",
    "print(f\"  Score: {complex_task_result.get('task_adherence', 'N/A')}\")\n",
    "print(f\"  Result: {complex_task_result.get('task_adherence_result', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d0037",
   "metadata": {},
   "source": [
    "## Step 9: Batch Evaluation\n",
    "\n",
    "In real-world applications, you'll want to evaluate multiple agent interactions at once. Let's create a comprehensive batch evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87bd280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 4 Zava shopping scenarios\n"
     ]
    }
   ],
   "source": [
    "# Create multiple evaluation scenarios for Zava shopping assistant\n",
    "evaluation_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Product Inquiry - Paint\",\n",
    "        \"query\": \"What paint do you recommend for a kitchen?\",\n",
    "        \"response\": (\n",
    "            \"For kitchens, I'd recommend our Interior Semi-Gloss Paint (SKU: PFIP000003, $47). \"\n",
    "            \"It's washable, moisture-resistant, and perfect for kitchens and bathrooms. \"\n",
    "            \"We currently have 2 units in stock.\"\n",
    "        ),\n",
    "        \"expected_intent\": \"product_recommendation\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Stock Check - Specific Product\", \n",
    "        \"query\": \"Is the Professional Claw Hammer in stock?\",\n",
    "        \"response\": (\n",
    "            \"Yes! The Professional Claw Hammer 16oz (SKU: HTHM001600) is in stock. \"\n",
    "            \"We have 25 units available at $28 each.\"\n",
    "        ),\n",
    "        \"expected_intent\": \"inventory_check\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Product Comparison - Tools\",\n",
    "        \"query\": \"What's the difference between your screwdriver sets?\",\n",
    "        \"response\": (\n",
    "            \"We have several options: Our Phillips Screwdriver Set ($16) features magnetic tips and cushion grips, \"\n",
    "            \"while the Flathead Set ($14) has precision-machined tips. \"\n",
    "            \"For electronics work, our Precision Screwdriver Kit ($22) is perfect for small appliances and eyeglasses.\"\n",
    "        ),\n",
    "        \"expected_intent\": \"product_comparison\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Off-Topic Response\",\n",
    "        \"query\": \"Do you sell hammers?\",\n",
    "        \"response\": (\n",
    "            \"Did you know that Zava was founded in 1995? \"\n",
    "            \"We're proud to serve DIY enthusiasts across the nation!\"\n",
    "        ),\n",
    "        \"expected_intent\": \"product_availability\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(evaluation_scenarios)} Zava shopping scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b2d753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BATCH AGENT EVALUATION RESULTS\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Scenario 1: Product Inquiry - Paint\n",
      "Query: What paint do you recommend for a kitchen?\n",
      "Intent Score: 5.0 (pass)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Scenario 2: Stock Check - Specific Product\n",
      "Query: Is the Professional Claw Hammer in stock?\n",
      "Intent Score: 5.0 (pass)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Scenario 2: Stock Check - Specific Product\n",
      "Query: Is the Professional Claw Hammer in stock?\n",
      "Intent Score: 5.0 (pass)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Scenario 3: Product Comparison - Tools\n",
      "Query: What's the difference between your screwdriver sets?\n",
      "Intent Score: 5.0 (pass)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Scenario 3: Product Comparison - Tools\n",
      "Query: What's the difference between your screwdriver sets?\n",
      "Intent Score: 5.0 (pass)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Scenario 4: Off-Topic Response\n",
      "Query: Do you sell hammers?\n",
      "Intent Score: 5.0 (pass)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Scenario 4: Off-Topic Response\n",
      "Query: Do you sell hammers?\n",
      "Intent Score: 1.0 (fail)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "Average Intent Resolution Score: 4.00\n",
      "Intent Resolution Pass Rate: 3/4 (75.0%)\n",
      "Intent Score: 1.0 (fail)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "Average Intent Resolution Score: 4.00\n",
      "Intent Resolution Pass Rate: 3/4 (75.0%)\n"
     ]
    }
   ],
   "source": [
    "# Run batch evaluation\n",
    "print(\"üìä BATCH AGENT EVALUATION RESULTS\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for i, scenario in enumerate(evaluation_scenarios, 1):\n",
    "    print(f\"\\nScenario {i}: {scenario['name']}\")\n",
    "    print(f\"Query: {scenario['query']}\")\n",
    "    \n",
    "    # Evaluate intent resolution\n",
    "    intent_result = intent_evaluator(\n",
    "        query=scenario['query'],\n",
    "        response=scenario['response']\n",
    "    )\n",
    "    \n",
    "    result_summary = {\n",
    "        'scenario': scenario['name'],\n",
    "        'intent_score': intent_result.get('intent_resolution', 0),\n",
    "        'intent_result': intent_result.get('intent_resolution_result', 'unknown')\n",
    "    }\n",
    "    \n",
    "    evaluation_results.append(result_summary)\n",
    "    \n",
    "    print(f\"Intent Score: {result_summary['intent_score']} ({result_summary['intent_result']})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "intent_scores = [r['intent_score'] for r in evaluation_results if isinstance(r['intent_score'], (int, float))]\n",
    "passed_intent = len([r for r in evaluation_results if r['intent_result'] == 'pass'])\n",
    "\n",
    "print(f\"Average Intent Resolution Score: {sum(intent_scores)/len(intent_scores):.2f}\")\n",
    "print(f\"Intent Resolution Pass Rate: {passed_intent}/{len(evaluation_results)} ({passed_intent/len(evaluation_results)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4461b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You've successfully learned how to evaluate AI agents using Azure AI Foundry's specialized evaluators! \n",
    "\n",
    "### What You Accomplished\n",
    "- Used **IntentResolutionEvaluator** to measure intent understanding\n",
    "- Assessed **ToolCallAccuracyEvaluator** for correct tool selection\n",
    "- Applied **TaskAdherenceEvaluator** to ensure agents stay within scope\n",
    "- Evaluated complex multi-step agent conversations\n",
    "- Created batch evaluation workflows for multiple scenarios\n",
    "\n",
    "### Key Takeaways\n",
    "- Agent evaluators provide specialized metrics for agentic workflows beyond simple query-response\n",
    "- Binary pass/fail results with detailed reasoning help identify specific improvement areas\n",
    "- Tool evaluation is crucial for agents that interact with external systems\n",
    "- Task adherence ensures agents maintain their intended purpose and boundaries\n",
    "- Combining quality and agent evaluators provides comprehensive assessment\n",
    "\n",
    "### Production Best Practices\n",
    "1. **Continuous Evaluation** - Set up automated evaluation pipelines for agent deployments\n",
    "2. **Threshold Monitoring** - Configure alerts when scores drop below acceptable levels\n",
    "3. **A/B Testing** - Compare different agent configurations using evaluation metrics\n",
    "4. **User Feedback Integration** - Combine automated evaluations with human feedback\n",
    "5. **Tool Coverage Testing** - Ensure all available tools are properly tested\n",
    "\n",
    "Great work! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
